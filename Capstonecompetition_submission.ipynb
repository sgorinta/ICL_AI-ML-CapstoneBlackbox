{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2lTv-F8GN5h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 1"
      ],
      "metadata": {
        "id": "v0--L7xxGnOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 1: Searching for Contamination Sources\n",
        "This may sound simple because you only have a two-dimensional input, however it is a very difficult problem.\n",
        "It corresponds to trying to find the source of radiation in some square area.\n",
        "However, you can only detect the radiation once you are very close to it, meaning most of the readings will\n",
        "be zero. There are two sources, one is not too dangerous, so make sure you try to find both modes of the\n",
        "function."
      ],
      "metadata": {
        "id": "21mOWuaDwM8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor"
      ],
      "metadata": {
        "id": "oN5dQPl2Gqpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "K__C48gjG2Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_1/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_1/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "wXsYGzVyGwSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "PyJvUQawxHJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_max = np.max(Y)\n",
        "print(\"Max output: \", y_max, \"which corresponds: \", X[np.where(Y == y_max)][0])"
      ],
      "metadata": {
        "id": "jzj52VBmH2IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the inputs first"
      ],
      "metadata": {
        "id": "e4RLnIGuIGEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_figheight(5)\n",
        "fig.set_figwidth(8)\n",
        "plt.scatter(X[:,0],X[:,1],c=Y)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "xJ9JmIhfILI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel as W\n",
        "\n",
        "kernel = C(0.1, (1e-3, 1e1)) * RBF(length_scale=0.5, length_scale_bounds=(1e-2, 1e0)) + W(noise_level=1e-4, noise_level_bounds=(1e-5, 1e-2))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=11)\n",
        "gpr.fit(X, Y)"
      ],
      "metadata": {
        "id": "ErarEhfZTUtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "######Acquisition Function 1 - Upper Confidence Bound####################\n",
        "x1 = np.linspace(X[np.where(Y == y_max)][0][0]-0.07, X[np.where(Y == y_max)][0][0]+0.07, 2000)\n",
        "x2 = np.linspace(X[np.where(Y == y_max)][0][1]-0.07, X[np.where(Y == y_max)][0][1]+0.07, 2000)\n",
        "X_grid = []\n",
        "for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "        X_grid.append([x1[i], x2[j]])\n",
        "X_grid = np.array(X_grid)\n",
        "\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb = mean + 0.76 * std\n",
        "\n",
        "idx_max = np.argmax(ucb)\n",
        "UB_NextQuery = X_grid[idx_max]\n",
        "print(\"UCB - Next Query: \", UB_NextQuery)\n",
        "\n",
        "####### Acquisition Function 2 - Probability of Improvement ################\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - y_max) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "PI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query: \", PI_NextQuery)\n",
        "\n",
        "########## Acquisition Function 3 - Expected Improvement############\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - f_best) / sigma\n",
        "    ei = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "EI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement - Next Query: \", EI_NextQuery)\n",
        "\n"
      ],
      "metadata": {
        "id": "pwbOobA3u_F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 2"
      ],
      "metadata": {
        "id": "byftOb7TkGQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 2 - Optimising Noisy Models\n",
        "Problem Description This corresponds to trying to optimise an unknown machine learning model. However, the initialization of the model is very important,\n",
        "meaning your observations will be very noisy, and the problem might have a lot of local optima! You are trying to make the model’s log-likelihood as large as possible.\n",
        "\n",
        "Acquisition Function found to be the most efficient over time: Expected Improvement with Knowledge Gradient\n",
        "\n",
        "Kernel Idea (evolved over time from simple default kernel usage to parametrized kernel based on inputs/outputs and function context):\n",
        "\n",
        "The Matérn kernel is a generalization of the RBF and the absolute exponential kernel, and it has an additional parameter that controls the smoothness of the resulting function.\n",
        "This can be particularly useful in our case, where the function may not be very smooth due to the presence of many local optima. Gaussian Process Regression also assumes that your function\n",
        "observations are noisy, so we might want to include a White Noise kernel in our model to account for measurement noise. This is especially important if our model’s log-likelihood evaluations\n",
        "are noisy.\n",
        "\n",
        "The Matern kernel is specified with a length_scale and nu (set to 1.5). WhiteKernel is used to model the nose. ConstantKernel is used as a scaling factor."
      ],
      "metadata": {
        "id": "mN21ZAzH43xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iAnOotNwIK7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_2/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_2/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "8NFPE3dEkKUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n"
      ],
      "metadata": {
        "id": "rbhxTTimmJWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "id": "S0szsRJJmVtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_max = np.max(Y)\n",
        "print(\"Max output: \", y_max, \"which corresponds: \", X[np.where(Y == y_max)][0])"
      ],
      "metadata": {
        "id": "1yRSDs4dNa4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_figheight(5)\n",
        "fig.set_figwidth(8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c = Y)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "61SCyVRaXjxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel as W\n",
        "\n",
        "kernel = C(1.0, (1e-3, 1e2)) * Matern(length_scale=0.5, length_scale_bounds=(1e-2, 1e1)) + W(noise_level=1, noise_level_bounds=(1e-5, 1e1))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=11)\n",
        "gpr.fit(X, Y)\n"
      ],
      "metadata": {
        "id": "Xo6YXoPYWMUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "######## Acquisition Function 1 - Upper Confidence Bound ##########\n",
        "x1 = np.linspace(X[np.where(Y == y_max)][0][0]-0.07, X[np.where(Y == y_max)][0][0]+0.07, 2000)\n",
        "x2 = np.linspace(X[np.where(Y == y_max)][0][1]-0.07, X[np.where(Y == y_max)][0][1]+0.07, 2000)\n",
        "X_grid = []\n",
        "for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "        X_grid.append([x1[i], x2[j]])\n",
        "X_grid = np.array(X_grid)\n",
        "\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb1 = mean + 0.86 * std\n",
        "\n",
        "idx_max = np.argmax(ucb1)\n",
        "UB_NextQuery = X_grid[idx_max]\n",
        "\n",
        "print(\"UCB - Next Query - Idea 1: \", UB_NextQuery)\n",
        "\n",
        "########## Acquisition Function 2 - Probability of Improvement###########################\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - y_max) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "PI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query: \", PI_NextQuery)\n",
        "\n",
        "################# Acquisition Function 3 - Expected Improvement##################\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - f_best) / sigma\n",
        "    ei = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "x1 = np.linspace(X[np.where(Y == y_max)][0][0]-0.1, X[np.where(Y == y_max)][0][0]+0.07, 1000)\n",
        "x2 = np.linspace(X[np.where(Y == y_max)][0][1]-0.1, X[np.where(Y == y_max)][0][1]+0.07, 1000)\n",
        "X_grid = np.array([[a, b] for a in x1 for b in x2])\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "EI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement - Next Query: \", EI_NextQuery)"
      ],
      "metadata": {
        "id": "mpHvza_-j3bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 3"
      ],
      "metadata": {
        "id": "wBYvQSi5efXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 3 - Drug Discovery Problem\n",
        "Problem Description In this example, you are doing drug discovery! You can select three compounds to create a drug, and receive a measurement of the people’s adverse reaction to the drug.\n",
        "You want to make this as close as possible to zero. (hint: one of the variables may not cause any effects on the person).\n"
      ],
      "metadata": {
        "id": "ZS6BASx_GVlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BhzWpar7ejWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_3/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_3/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "mOLEfGiteqPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_max = np.max(Y)\n",
        "print(\"Max output: \", y_max, \"which corresponds: \", X[np.where(Y == y_max)][0])"
      ],
      "metadata": {
        "id": "GO0D6Y98QPJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=Y)\n",
        "\n",
        "ax.set_xlabel('X Label')\n",
        "ax.set_ylabel('Y Label')\n",
        "ax.set_zlabel('Z Label')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aDg7knvFe6y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "gprZ8e55fDgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel as W\n",
        "\n",
        "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=0.1, length_scale_bounds=(1e-3, 1e1), nu=2.5) + W(noise_level=0.1, noise_level_bounds=(1e-10, 1e-1))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=11)\n",
        "\n",
        "gpr.fit(X, Y)"
      ],
      "metadata": {
        "id": "0JIcRec0QsQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############Acquisition Function 1 - Upper Confidence Bound################\n",
        "x1 = np.linspace(0.73, 0.80, 250)\n",
        "x2 = np.linspace(0.57, 0.65, 250)\n",
        "x3 = np.linspace(0.40, 0.48, 250)\n",
        "\n",
        "X_grid = []\n",
        "for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "        for k in range(len(x3)):\n",
        "            X_grid.append([x1[i], x2[j], x3[k]])\n",
        "\n",
        "X_grid = np.array(X_grid)\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb1 = mean + 0.86 * std\n",
        "\n",
        "idx_max = np.argmax(ucb1)\n",
        "UB_NextQuery = X_grid[idx_max]\n",
        "print(\"UCB - Next Query - Idea 1: \", UB_NextQuery)\n",
        "\n",
        "\n",
        "###### Acquisition Function 2 - Probability of Improvement\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - y_max) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "PI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query - Idea 1: \", PI_NextQuery)\n",
        "\n",
        "\n",
        "###### Acquisition Function 3 - Thompson Sampling\n",
        "x1 = np.linspace(0.73, 0.80, 150)\n",
        "x2 = np.linspace(0.57, 0.65, 150)\n",
        "x3 = np.linspace(0.40, 0.48, 150)\n",
        "\n",
        "X_grid = []\n",
        "for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "        for k in range(len(x3)):\n",
        "            X_grid.append([x1[i], x2[j], x3[k]])\n",
        "\n",
        "def compute_thompson(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    sample = np.random.normal(mu, sigma)\n",
        "    return sample\n",
        "\n",
        "thompson_values = [compute_thompson(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(thompson_values)\n",
        "TS_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"Thompson Sampling - Next Query: \", TS_NextQuery)\n",
        "\n",
        "###### Acquisition Function 4 - Bayesian Expected Losses\n",
        "\n",
        "def compute_expected_loss(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    z = (mu - y_max) / sigma\n",
        "    expected_loss = (mu - y_max) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return expected_loss\n",
        "\n",
        "expected_loss_values = [compute_expected_loss(x) for x in X_grid]\n",
        "\n",
        "\n",
        "next_idx = np.argmin(expected_loss_values)\n",
        "BL_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"Bayesian Expected Loss - Next Query: \", BL_NextQuery)\n",
        "\n",
        "######## Acquisition Function 5 - Expected Improvement\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - f_best) / sigma\n",
        "    ei = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "EI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement - Next Query: \", EI_NextQuery)"
      ],
      "metadata": {
        "id": "No1FocC5Q44_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 4"
      ],
      "metadata": {
        "id": "iW8IjQakNyFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 4: Fast, but Inaccurate Modelling\n",
        "This example is for a particular business relying heavily on online sales.\n",
        "It can run very accurate calculations to figure out what is the optimal placement of their product\n",
        "across warehouses. Unfortunately, the calculations are extremely expensive (computationally) to run,\n",
        "so they can only do it once every two weeks. Instead, they propose using a machine learning model which\n",
        "approximates the solution quickly (in a few hours). The model has four hyper-parameters you need to tune,\n",
        "and the output corresponds to the difference between the expensive calculation, and the model.\n",
        "Since you are modelling a dynamical system, expect a lot of local optima!"
      ],
      "metadata": {
        "id": "gsfCxzGSN069"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_4/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_4/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "ko7axmGQOAAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "cgoRIVHqOHu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=0.1)\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=11)\n",
        "\n",
        "gpr.fit(X, Y)"
      ],
      "metadata": {
        "id": "P5OTcbYVIuq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### Acquisition Function 1 - Upper Confidence Bound##########\n",
        "\n",
        "import itertools as it\n",
        "\n",
        "x_range = np.linspace(0, 1, 50)\n",
        "\n",
        "dim = 4\n",
        "X_grid = np.meshgrid(*([x_range] * dim))\n",
        "X_grid = np.stack(X_grid, axis=-1).reshape(-1, dim)\n",
        "\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb1 = mean + 0.90 * std\n",
        "\n",
        "idx_max = np.argmax(ucb1)\n",
        "UB_NextQuery = X_grid[idx_max]\n",
        "print(\"UCB - Next Query: \", UB_NextQuery)\n",
        "\n",
        "####### Acquisition Function 2 - Probability of Improvement########\n",
        "from scipy.stats import norm\n",
        "\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - y_max) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "PI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query: \", PI_NextQuery)\n",
        "\n",
        "###### Acquisition Function - Expected Improvement#########\n",
        "from scipy.stats import norm\n",
        "\n",
        "x_range = np.linspace(0, 1, 40)\n",
        "\n",
        "dim = 4\n",
        "X_grid = np.meshgrid(*([x_range] * dim))\n",
        "X_grid = np.stack(X_grid, axis=-1).reshape(-1, dim)\n",
        "\n",
        "\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - y_max) / sigma\n",
        "    ei = (mu - y_max) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "EI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement - Next Query: \", EI_NextQuery)"
      ],
      "metadata": {
        "id": "xmVNIdnsIzoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 5"
      ],
      "metadata": {
        "id": "uhtVKyPLbar0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 5: Yield in a Chemical Reaction\n",
        "This time you are trying to optimise another four-dimensional black-box.\n",
        "It corresponds to the yield of a chemical process after processing in some factory.\n",
        "This type of process tends to be unimodal.\n",
        "Try to find the combination of chemicals that maximizes the yield!"
      ],
      "metadata": {
        "id": "qor6J7ERbkzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_5/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_5/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "cMBKXoDjbxBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "qQWwKA33b1pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(Y))"
      ],
      "metadata": {
        "id": "CH5dhy8eRxKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "kernel_matern = C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, nu=1.5)  # Adjust nu for smoothness\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel_matern, n_restarts_optimizer=9)\n",
        "\n",
        "gpr.fit(X, Y)"
      ],
      "metadata": {
        "id": "AacZGR47N26k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Acquisition Function 1 - Upper Confidence Bound ########\n",
        "import itertools as it\n",
        "\n",
        "x_range = np.linspace(0, 1, 50)\n",
        "\n",
        "dim = 4\n",
        "X_grid = np.meshgrid(*([x_range] * dim))\n",
        "X_grid = np.stack(X_grid, axis=-1).reshape(-1, dim)\n",
        "\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb1 = mean + 1.10 * std\n",
        "\n",
        "idx_max = np.argmax(ucb1)\n",
        "UB_NextQuery = X_grid[idx_max]\n",
        "print(\"UCB - Next Query\", UB_NextQuery)\n",
        "\n",
        "######## Acquisition Function 2 - Probability of Improvement #########\n",
        "f_best = np.max(Y)\n",
        "\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - f_best) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "PI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query\", PI_NextQuery)\n",
        "\n",
        "####### Acquisition Function 3  - Expected Improvement ###################\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.stats import norm\n",
        "\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - f_best) / sigma\n",
        "    ei = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "EI_NextQuery = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement - Next Query: \", EI_NextQuery)"
      ],
      "metadata": {
        "id": "GVYnqcRNODdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6FHGho5OBEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 6"
      ],
      "metadata": {
        "id": "oxkyKQvQsWUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 6: Cake and Stuff\n",
        "Time to get cooking! You are optimising a cake recipe. There are five ingredients.\n",
        "The outputs correspond to the sum of different objectives: flavor, consistency, calories, waste and cost.\n",
        "Each objective receives negative points by our expert taster.\n",
        "You want this sum to be as close to zero as possible!"
      ],
      "metadata": {
        "id": "D5dlKnybVBLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q-1gXFFlsZqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_6/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_6/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "HMi4ieRwu8GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "pSE_k7qPvJ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(Y))\n",
        "print(np.min(Y))"
      ],
      "metadata": {
        "id": "MkkPuC6ZapSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=[1.0, 0.5, 2.0, 0.8, 1.5])\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=11)\n",
        "gpr.fit(X, Y)"
      ],
      "metadata": {
        "id": "wFLZAmccOgF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### Acquisition Function 1 - Upper Confidence Bound\n",
        "import itertools as it\n",
        "\n",
        "x1 = np.linspace(0, 1, 30)\n",
        "\n",
        "dim = 5\n",
        "X_grid = np.fromiter(it.chain(*it.product(x1, repeat=dim)), dtype=float).reshape(-1,dim)\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb1 = mean + 1.56 * std\n",
        "\n",
        "idx_max = np.argmax(ucb1)\n",
        "next_query = X_grid[idx_max]\n",
        "print(\"UCB - Next Query\", next_query)\n",
        "\n",
        "######### Acquisition Function 2 - Probability of Improvement#########\n",
        "from scipy.stats import norm\n",
        "\n",
        "f_best = np.max(Y)\n",
        "\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - f_best) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query\", next_query)\n",
        "\n",
        "######### Acquisition Function 3 - Expected Improvement ##########\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - f_best) / sigma\n",
        "    ei = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement\", next_query)"
      ],
      "metadata": {
        "id": "Khwhdpf8QOUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 7\n"
      ],
      "metadata": {
        "id": "2Yq_xSWG1nnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 7: Sometimes Lazy is Best\n",
        "You are now optimising six hyper-parameters of a machine learning model.\n",
        "Note that it is a popular and frequently used model, so maybe you could search to see if anyone\n",
        "else has optisized it before?"
      ],
      "metadata": {
        "id": "b4QLUWyzbSbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Nvbco-xN0NPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_7/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_7/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "hNCJhynO1tTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "4ZdFNwSh1xc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(Y))\n",
        "print(np.min(Y))"
      ],
      "metadata": {
        "id": "HjvDL0YLb6hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_max = np.max(Y)\n",
        "print(\"Max output: \", y_max, \"which corresponds to: \", X[np.where(Y == y_max)][0])"
      ],
      "metadata": {
        "id": "kQFBMpq2glsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####Build a Bayesian Model\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern\n",
        "\n",
        "kernel_matern = 1.0 * Matern(length_scale=1.0, nu=1.5)  # Adjust nu for smoothness\n",
        "gpr = GaussianProcessRegressor(kernel=kernel_matern, n_restarts_optimizer=10)\n",
        "gpr.fit(X, Y)"
      ],
      "metadata": {
        "id": "KThphznDgn63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######Acquisition Function 1 - Upper Confidence Bound\n",
        "from scipy.stats import norm\n",
        "import itertools as it\n",
        "\n",
        "x1 = np.linspace(0, 1, 15)\n",
        "\n",
        "list(it.product(['1','2','3','4',], ['a', 'b','c','d']))\n",
        "\n",
        "dim = 6\n",
        "X_grid = np.fromiter(it.chain(*it.product(x1, repeat=dim)), dtype=float).reshape(-1,dim)\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb1 = mean + 1.16 * std\n",
        "\n",
        "idx_max = np.argmax(ucb1)\n",
        "next_query = X_grid[idx_max]\n",
        "print(\"UCB - Next Query: \", next_query)\n",
        "\n",
        "########Acquisition Function 2 - Probability of Improvement\n",
        "from scipy.stats import norm\n",
        "\n",
        "f_best = np.max(Y)\n",
        "\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - f_best) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query\", next_query)\n",
        "\n",
        "######Acquisition Function 3 - Thompson Sampling\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.stats import norm\n",
        "\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gpr.fit(X, Y)\n",
        "\n",
        "def compute_thompson(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    sample = np.random.normal(mu, sigma)\n",
        "    return sample\n",
        "\n",
        "thompson_values = [compute_thompson(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(thompson_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"Thompson Sampling\", next_query)\n",
        "\n",
        "#######Acquisition Function 4 - Bayesian Expected Losses\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gpr.fit(X, Y)\n",
        "\n",
        "def compute_expected_loss(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    z = (mu - f_best) / sigma\n",
        "    expected_loss = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return expected_loss\n",
        "\n",
        "expected_loss_values = [compute_expected_loss(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmin(expected_loss_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"Bayesian Expected Loss\", next_query)\n",
        "\n",
        "########Acquisition Function 5 - Expected Improvement\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - f_best) / sigma\n",
        "    ei = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement\", next_query)"
      ],
      "metadata": {
        "id": "Z74jon1Zgrka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function 8"
      ],
      "metadata": {
        "id": "pNDwjg7H6LXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Function 8: High-dimensional Optimisation\n",
        "You’ve reach the final, 8-dimensional search space.\n",
        "High-dimensional black-box optimisation can be very difficult, so sticking to local solutions is not\n",
        "the worst idea here."
      ],
      "metadata": {
        "id": "uga0pNAacmkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "file_inputs  = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_8/initial_inputs.npy'\n",
        "file_outputs = '/content/drive/My Drive/Colab Notebooks/IMP-PCMLAI-capstone-initial_data/initial_data/function_8/initial_outputs.npy'\n",
        "\n",
        "X = np.load(file_inputs)\n",
        "Y = np.load(file_outputs)"
      ],
      "metadata": {
        "id": "6WQzxbn86N7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "P-WH99-j6dd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_max = np.max(Y)\n",
        "print(\"Max output: \", y_max, \"which corresponds to: \", X[np.where(Y == y_max)][0])"
      ],
      "metadata": {
        "id": "m0QeDmCBiXt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### Build a Bayesian Model\n",
        "# Create a Gaussian Process Regressor with the specified kernel\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel as W\n",
        "\n",
        "# Adjust the kernel parameters to suit the scale of the problem\n",
        "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=0.1, length_scale_bounds=(1e-3, 1e1), nu=2.5) + W(noise_level=0.1, noise_level_bounds=(1e-10, 1e-1))\n",
        "\n",
        "# Create the Gaussian Process Regressor model\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=10)\n",
        "\n",
        "gpr.fit(X, Y)"
      ],
      "metadata": {
        "id": "jqI40Udssfk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########Acquisition Function 1 - Upper Confidence Bound\n",
        "import itertools as it\n",
        "\n",
        "x1 = np.linspace(0, 1, 8)\n",
        "\n",
        "dim = 8\n",
        "X_grid = np.fromiter(it.chain(*it.product(x1, repeat=dim)), dtype=float).reshape(-1,dim)\n",
        "mean, std = gpr.predict(X_grid, return_std = True)\n",
        "\n",
        "ucb1 = mean + 0.96 * std\n",
        "\n",
        "idx_max = np.argmax(ucb1)\n",
        "next_query = X_grid[idx_max]\n",
        "print(\"UCB - Next Query: \", next_query)"
      ],
      "metadata": {
        "id": "PVUjTd43icia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Acquisition Function 2 - Probability of Improvement\n",
        "from scipy.stats import norm\n",
        "\n",
        "f_best = np.max(Y)\n",
        "\n",
        "def compute_pi(x):\n",
        "    mu, sigma = gpr.predict(X_grid, return_std = True)\n",
        "    z = (mu - f_best) / sigma\n",
        "    pi = norm.cdf(z)\n",
        "    return pi\n",
        "\n",
        "pi_values = compute_pi(X_grid)\n",
        "\n",
        "next_idx = np.argmax(pi_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"PI - Next Query: \", next_query)"
      ],
      "metadata": {
        "id": "wO8ojp3GijfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######Acquisition Function - Thompson Sampling\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.stats import norm\n",
        "\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gpr.fit(X, Y)\n",
        "\n",
        "def compute_thompson(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    sample = np.random.normal(mu, sigma)\n",
        "    return sample\n",
        "\n",
        "thompson_values = [compute_thompson(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(thompson_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"Thompson Sampling - Next Query: \", next_query)"
      ],
      "metadata": {
        "id": "sQF5PgU2imY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Acquisition Function - Bayesian Expected Losses\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gpr.fit(X, Y)\n",
        "\n",
        "def compute_expected_loss(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    z = (mu - f_best) / sigma\n",
        "    expected_loss = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return expected_loss\n",
        "\n",
        "\n",
        "expected_loss_values = [compute_expected_loss(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmin(expected_loss_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"Bayesian Expected Loss - Next Query: \", next_query)"
      ],
      "metadata": {
        "id": "g9OQPtcBipQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### Acquisition Function - Expected Improvement\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.stats import norm\n",
        "\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gpr.fit(X, Y)\n",
        "\n",
        "def compute_expected_improvement(x):\n",
        "    mu, sigma = gpr.predict([x], return_std=True)\n",
        "    f_best = np.max(Y)\n",
        "    z = (mu - f_best) / sigma\n",
        "    ei = (mu - f_best) * norm.cdf(z) + sigma * norm.pdf(z)\n",
        "    return ei\n",
        "\n",
        "ei_values = [compute_expected_improvement(x) for x in X_grid]\n",
        "\n",
        "next_idx = np.argmax(ei_values)\n",
        "next_query = X_grid[next_idx]\n",
        "\n",
        "print(\"Expected Improvement - Next Query: \", next_query)"
      ],
      "metadata": {
        "id": "gSi8LZRyisY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}